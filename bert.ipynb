{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "26dab062-5c11-40d5-b4de-c769dc53a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings Shape: torch.Size([20, 11, 786])\n",
      "Position Embeddings Shape: torch.Size([20, 11, 786])\n",
      "Final Input Embeddings Shape: torch.Size([20, 11, 786])\n",
      "\n",
      "\n",
      "Q Shape: torch.Size([20, 11, 786])\n",
      "K Shape: torch.Size([20, 11, 786])\n",
      "V Shape: torch.Size([20, 11, 786])\n",
      "Attention Scores Shape: torch.Size([20, 11, 786])\n",
      "\n",
      "\n",
      "Residual Matrix Shape: torch.Size([20, 11, 786])\n",
      "Layer Norm 1 Output Shape: torch.Size([20, 11, 786])\n",
      "\n",
      "\n",
      "FFN Layer 1 Weight Shape: torch.Size([16, 786])\n",
      "FFN Layer 1 Bias Shape: torch.Size([16])\n",
      "FFN Layer 3 Weight Shape: torch.Size([786, 16])\n",
      "FFN Layer 3 Bias Shape: torch.Size([786])\n",
      "FFN Output Shape: torch.Size([20, 11, 786])\n",
      "\n",
      "\n",
      "Encoder 1 Output Shape: torch.Size([20, 11, 786])\n",
      "Q Shape: torch.Size([20, 11, 786])\n",
      "K Shape: torch.Size([20, 11, 786])\n",
      "V Shape: torch.Size([20, 11, 786])\n",
      "Attention Scores Shape: torch.Size([20, 11, 786])\n",
      "\n",
      "\n",
      "Residual Matrix Shape: torch.Size([20, 11, 786])\n",
      "Layer Norm 1 Output Shape: torch.Size([20, 11, 786])\n",
      "\n",
      "\n",
      "FFN Layer 1 Weight Shape: torch.Size([16, 786])\n",
      "FFN Layer 1 Bias Shape: torch.Size([16])\n",
      "FFN Layer 3 Weight Shape: torch.Size([786, 16])\n",
      "FFN Layer 3 Bias Shape: torch.Size([786])\n",
      "FFN Output Shape: torch.Size([20, 11, 786])\n",
      "\n",
      "\n",
      "Encoder 2 Output Shape: torch.Size([20, 11, 786])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sentences = [\n",
    "    \"The product arrived damaged and is unusable.\",\n",
    "    \"I received the wrong item in my order.\",\n",
    "    \"The delivery was significantly delayed.\",\n",
    "    \"The customer service representative was rude and unhelpful.\",\n",
    "    \"I was charged for an item I did not purchase.\",\n",
    "    \"The product quality did not meet my expectations.\",\n",
    "    \"My order was incomplete and missing several items.\",\n",
    "    \"I encountered technical issues while using the product.\",\n",
    "    \"The packaging was inadequate and caused the product to break.\",\n",
    "    \"The website was difficult to navigate and place an order.\",\n",
    "    \"I was overcharged for shipping and handling.\",\n",
    "    \"The product I received was different from what was advertised.\",\n",
    "    \"I did not receive a confirmation email for my order.\",\n",
    "    \"The return process was too complicated and time-consuming.\",\n",
    "    \"I had trouble reaching customer support via phone.\",\n",
    "    \"The product was defective and stopped working after a few uses.\",\n",
    "    \"I was not informed about the additional fees before purchasing.\",\n",
    "    \"The item was missing from my shipment.\",\n",
    "    \"The store's policies were not clearly stated on the website.\",\n",
    "    \"The quality of customer support was poor and not satisfactory.\"\n",
    "]\n",
    "\n",
    "vocab = list(set(' '.join(sentences).split()))\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "d_model = 786\n",
    "d_ff = 16\n",
    "seq_len = max(len(sentence.split()) for sentence in sentences)\n",
    "batch_size = len(sentences)\n",
    "tokenized_sentences = [[word_to_idx[word] for word in sentence.split()] for sentence in sentences]\n",
    "padded_sentences = [tokens + [0] * (seq_len - len(tokens)) for tokens in tokenized_sentences]\n",
    "input_ids = torch.tensor(padded_sentences)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, seq_len):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embeddings = nn.Embedding(seq_len, d_model)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        word_embeds = self.word_embeddings(input_ids)\n",
    "        position_ids = torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        embeddings = word_embeds + position_embeds\n",
    "\n",
    "        print(\"Word Embeddings Shape:\", word_embeds.shape)\n",
    "        print(\"Position Embeddings Shape:\", position_embeds.shape)\n",
    "        print(\"Final Input Embeddings Shape:\", embeddings.shape)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = nn.Linear(d_model, d_model * 3) \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        QKV = self.attention(x)\n",
    "        Q, K, V = torch.chunk(QKV, 3, dim=-1)\n",
    "\n",
    "        print(\"Q Shape:\", Q.shape)\n",
    "        print(\"K Shape:\", K.shape)\n",
    "        print(\"V Shape:\", V.shape)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "        attention_scores = torch.matmul(scores, V)\n",
    "\n",
    "        print(\"Attention Scores Shape:\", attention_scores.shape)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        residual = x + attention_scores\n",
    "        norm1_output = self.layer_norm1(residual)\n",
    "\n",
    "        print(\"Residual Matrix Shape:\", residual.shape)\n",
    "        print(\"Layer Norm 1 Output Shape:\", norm1_output.shape)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        ffn_output = self.ffn(norm1_output)\n",
    "\n",
    "        for idx, layer in enumerate(self.ffn):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                print(f\"FFN Layer {idx+1} Weight Shape: {layer.weight.shape}\")\n",
    "                print(f\"FFN Layer {idx+1} Bias Shape: {layer.bias.shape}\")\n",
    "\n",
    "        print(\"FFN Output Shape:\", ffn_output.shape)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        residual2 = norm1_output + ffn_output\n",
    "        norm2_output = self.layer_norm2(residual2)\n",
    "\n",
    "        return norm2_output\n",
    "\n",
    "class SimpleBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff, seq_len):\n",
    "        super(SimpleBERT, self).__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, d_model, seq_len)\n",
    "        self.encoder1 = EncoderLayer(d_model, d_ff)\n",
    "        self.encoder2 = EncoderLayer(d_model, d_ff)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeds = self.embeddings(input_ids)\n",
    "        encoder1_output = self.encoder1(embeds)\n",
    "        print(\"Encoder 1 Output Shape:\", encoder1_output.shape)\n",
    "        encoder2_output = self.encoder2(encoder1_output)\n",
    "        print(\"Encoder 2 Output Shape:\", encoder2_output.shape)\n",
    "\n",
    "        return encoder2_output\n",
    "\n",
    "model = SimpleBERT(vocab_size, d_model, d_ff, seq_len)\n",
    "output = model(input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
